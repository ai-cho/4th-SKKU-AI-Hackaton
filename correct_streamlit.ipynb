{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czcC5kBRdlgx",
        "outputId": "881f2869-0ad0-4d87-82c5-7835580df6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3ExedOKvz3F",
        "outputId": "71a9baa4-af85-4ff5-9383-fa64273b5ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.39.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (16.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.2)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<6,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .ipynb 파일 코드 통째로 넣음\n",
        "\n",
        "\n",
        "\n",
        "*   classification_using_image.ipynb\n",
        "*   classification_using_features.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "tqLzb5IMT6Zy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y1PklLvvrdA",
        "outputId": "86992e3f-f263-46bd-be07-b7164084b79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "# 밑 코드를 app.py에 넣어서 저장\n",
        "# 내용은 위의 2개 ipynb 파일과 동일\n",
        "# 코드 굳이 볼 필요 X (괜히 더 복잡)\n",
        "\n",
        "%%writefile app.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "import json\n",
        "import cv2 as cv\n",
        "import random\n",
        "from datetime import datetime\n",
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import streamlit as st\n",
        "import torch\n",
        "from PIL import Image\n",
        "import json\n",
        "import transformers\n",
        "\n",
        "CLASS = ['성충_날개불구바이러스감염증',\n",
        "        '성충_응애',\n",
        "        '성충_정상',\n",
        "        '유충_부저병',\n",
        "        '유충_석고병',\n",
        "        '유충_응애',\n",
        "        '유충_정상']\n",
        "IM_HEIGHT = 224\n",
        "IM_WIDTH = 224\n",
        "\n",
        "\n",
        "def read_data_1_input(json_file):\n",
        "\n",
        "    temp, hum, co2 = get_data(json_file)\n",
        "\n",
        "    temp = div_func(temp)\n",
        "    temp = normalize_resize_concat(temp, resize=True)\n",
        "\n",
        "    hum = div_func(hum)\n",
        "    hum = normalize_resize_concat(hum, resize=True)\n",
        "\n",
        "    co2 = div_func(co2)\n",
        "    co2 = normalize_resize_concat(co2, resize=True)\n",
        "\n",
        "    return cv.merge((temp, hum, co2))\n",
        "\n",
        "\n",
        "def get_data(annot_file):\n",
        "    with open(annot_file, \"r\", encoding='UTF-8-SIG') as f:\n",
        "        data = json.loads(f.read())\n",
        "\n",
        "    temp = np.array(data[\"environment\"]['in_temperature'], dtype='float')\n",
        "    humidity = np.array(data[\"environment\"]['in_humidity'], dtype='float')\n",
        "    co2 = np.array(data[\"environment\"]['in_carbon_monoxide'], dtype='float')\n",
        "\n",
        "    return temp, humidity, co2\n",
        "\n",
        "def normalize_resize_concat(x, hist=False, resize=False, merge=False):\n",
        "\n",
        "    x = cv.normalize(x, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\n",
        "    x = np.array(x, np.uint8)\n",
        "\n",
        "    if resize:\n",
        "        x = cv.resize(x, (IM_HEIGHT, IM_WIDTH), cv.INTER_AREA)\n",
        "\n",
        "    if hist:\n",
        "        x = cv.equalizeHist(x)\n",
        "\n",
        "    if merge:\n",
        "        x = cv.merge((x, x, x))\n",
        "\n",
        "    return x\n",
        "\n",
        "def get_prediction(model_file, raw_data):\n",
        "    data = read_data_1_input(raw_data)\n",
        "    data = preprocess_image(np_image=data, net_type='efficientnet')\n",
        "    prediction = model_file.predict(np.expand_dims(data, axis=0), batch_size=1)\n",
        "    return np.argmax(prediction)\n",
        "\n",
        "def make_model_efficientnet_1input():\n",
        "    m1 = tf.keras.applications.EfficientNetB0(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "    gap1 = tf.keras.layers.GlobalAveragePooling2D()(m1.output)\n",
        "    l3 = tf.keras.layers.Dense(7, activation='softmax')(gap1)\n",
        "    model = tf.keras.models.Model(inputs=m1.inputs, outputs=l3)\n",
        "    return model\n",
        "\n",
        "# 모델 불러오기\n",
        "def load_model1():\n",
        "  # MODEL LOADING\n",
        "    model = make_model_efficientnet_1input()\n",
        "    net_name = '/content/drive/MyDrive/제4회 AI교육 해커톤/network.hdf5'\n",
        "    model.load_weights(net_name)\n",
        "    # model = model.to('cpu')\n",
        "    return model\n",
        "\n",
        "def load_model2():\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = models.resnet18(pretrained=False)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = torch.nn.Linear(num_ftrs, 7)\n",
        "    model_path = '/content/drive/MyDrive/제4회 AI교육 해커톤/bee_classification_resnet.pth'\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    model = model.to('cpu')\n",
        "    model.eval()  # 추론 모드로 전환 (학습용이 아닌 추론용)\n",
        "    return model\n",
        "\n",
        "# JSON 파일 처리 함수\n",
        "def process_json_file(json_data, model):\n",
        "    prediction = get_prediction(model, json_data)\n",
        "    prediction = CLASS[prediction]\n",
        "    st.write(f\"Prediction: {prediction}\")\n",
        "    result = f\"Processed JSON with {model}\"\n",
        "    return result\n",
        "\n",
        "# 이미지 처리 함수\n",
        "def process_image(image, model):\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize((224, 224)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "  ])\n",
        "    input_tensor = preprocess(image)\n",
        "    input_batch = input_tensor.unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "    # 모델에 입력하여 추론 실행\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        output = model(input_batch)\n",
        "\n",
        "    # 소프트맥스 적용하여 확률 변환\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "\n",
        "    # 클래스 이름 정의 (학습 시 사용한 클래스와 동일하게)\n",
        "\n",
        "    # 가장 높은 확률의 클래스 예측\n",
        "    _, predicted_class = torch.max(probabilities, 0)\n",
        "\n",
        "    return CLASS[predicted_class]\n",
        "\n",
        "def preprocess_image(np_image, net_type='efficientnet'):\n",
        "    assert net_type in ['mobilenet', 'resnet50', 'efficientnet'], 'network not in the list'\n",
        "\n",
        "    m_image = tf.convert_to_tensor(np_image, dtype=tf.float32)\n",
        "\n",
        "    # if net_type == 'resnet50':\n",
        "    #     return preprocess_input_resnet50(m_image)\n",
        "\n",
        "    if net_type == 'efficientnet':\n",
        "        return m_image\n",
        "\n",
        "def read_data_1_input(json_file):\n",
        "\n",
        "    temp, hum, co2 = get_data(json_file)\n",
        "\n",
        "    temp = div_func(temp)\n",
        "    temp = normalize_resize_concat(temp, resize=True)\n",
        "\n",
        "    hum = div_func(hum)\n",
        "    hum = normalize_resize_concat(hum, resize=True)\n",
        "\n",
        "    co2 = div_func(co2)\n",
        "    co2 = normalize_resize_concat(co2, resize=True)\n",
        "\n",
        "    return cv.merge((temp, hum, co2))\n",
        "\n",
        "\n",
        "def get_data(annot_file):\n",
        "    data = json.load(annot_file)\n",
        "\n",
        "    temp = np.array(data[\"environment\"]['in_temperature'], dtype='float')\n",
        "    humidity = np.array(data[\"environment\"]['in_humidity'], dtype='float')\n",
        "    co2 = np.array(data[\"environment\"]['in_carbon_monoxide'], dtype='float')\n",
        "\n",
        "    return temp, humidity, co2\n",
        "\n",
        "\n",
        "def div_func(x):\n",
        "    x = x.squeeze()\n",
        "    x1 = x[:189]\n",
        "    for n in range(1, 189):\n",
        "        x1 = np.vstack((x1, x[n*189:(n+1)*189]))\n",
        "    return x1\n",
        "\n",
        "def normalize_resize_concat(x, hist=False, resize=False, merge=False):\n",
        "\n",
        "    x = cv.normalize(x, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\n",
        "    x = np.array(x, np.uint8)\n",
        "\n",
        "    if resize:\n",
        "        x = cv.resize(x, (IM_HEIGHT, IM_WIDTH), cv.INTER_AREA)\n",
        "\n",
        "    if hist:\n",
        "        x = cv.equalizeHist(x)\n",
        "\n",
        "    if merge:\n",
        "        x = cv.merge((x, x, x))\n",
        "\n",
        "    return x\n",
        "\n",
        "def make_model_efficientnet_1input():\n",
        "    m1 = tf.keras.applications.EfficientNetB0(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "    gap1 = tf.keras.layers.GlobalAveragePooling2D()(m1.output)\n",
        "    l3 = tf.keras.layers.Dense(7, activation='softmax')(gap1)\n",
        "    model = tf.keras.models.Model(inputs=m1.inputs, outputs=l3)\n",
        "    return model\n",
        "\n",
        "model2 = load_model2()  # 이미지를 처리하는 모델\n",
        "model2 = model2.to('cpu')\n",
        "\n",
        "# Streamlit 앱 시작\n",
        "st.title(\"JSON 또는 이미지 입력에 따른 모델 실행\")\n",
        "\n",
        "# 파일 업로드 기능\n",
        "uploaded_file = st.file_uploader(\"JSON 파일 또는 이미지 파일을 업로드하세요\", type=[\"json\", \"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "# llm으로 생성한 질병 설명 텍스트\n",
        "llm_results = {'성충_날개불구바이러스감염증': '날개불구바이러스감염증은 꿀벌의 체액을 빨아먹는 바로아 응애에 의해 전파되는 바이러스성 질병입니다. 이 질병으로 감염된 꿀벌은 날개 기형을 일으키고 비행 능력을 상실하게 됩니다. 군체 내에서 바이러스가 전염되면 꿀벌의 활력이 떨어지고 벌 개체 수가 급격히 감소하는 등 치명적인 결과를 초래할 수 있습니다. 이를 예방하고 치료하려면 응애를 제거하고 화학적 방법으로 항바이러스 치료를 수행하며 군체 관리를 철저히 해야 합니다.',\n",
        "                '성충_응애': '성충_응애는 꿀벌의 날개 기형을 일으키는 바이러스성 질병입니다. 이 질병은 바로아 응애가 꿀벌의 체액을 빨아먹으며 바이러스를 전달하는 과정에서 발생합니다. 감염된 꿀벌은 날개 기형을 일으키고 비행 능력을 상실하여 짧은 수명을 갖게 됩니다. 이를 방지하기 위해 응애 제거, 항바이러스 치료, 군체 관리 등을 통해 질병을 관리해야 합니다.',\n",
        "                '유충_부저병': '유충_부저병은 꿀벌의 유충이 부저에 감염되는 바이러스성 질병입니다. 이 질병은 꿀벌의 유충이 부저에 있는 바이러스에 감염되어 부저가 부식되거나 파괴되는 것을 일으킵니다. 이러한 질병은 꿀벌의 생산을 저해하고, 꿀벌의 생존을 위협합니다. 질병을 예방하고 치료하기 위해서는 꿀벌의 유충을 적절하게 관리하고, 부저를 청결하게 하는 것이 중요합니다.',\n",
        "                '유충_석고병': '유충_석고병은 꿀벌의 유충이 석고균에 감염된 질병입니다. 이 질병은 유충이 석고균에 의해 파괴되는 경우를 초래하고, 이에 따라 꿀벌의 생산이 저하됩니다. 석고균은 꿀벌의 유충을 죽이는 데 사용되는 화학 물질을 생산하는 데 사용됩니다. 이 질병은 꿀벌의 생산을 저하하고, 꿀벌의 건강을 악화시킬 수 있습니다.',\n",
        "                '유충_응애': '유충_응애는 꿀벌의 유충이 응애에 의해 감염되는 바이러스성 질병입니다. 이 질병은 꿀벌의 유충이 응애에 기생당하여 바이러스를 전달받아 감염됩니다. 감염된 꿀벌의 유충은 체력이 약해지게 되고, 성충이 되지 못하는 경우가 많습니다. 이 질병은 꿀벌 군체의 활력을 떨어뜨리고, 벌 개체 수를 감소시킬 수 있습니다.'}\n",
        "\n",
        "m_prediction = None\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    file_extension = uploaded_file.name.split('.')[-1].lower()\n",
        "\n",
        "    if file_extension == \"json\": # 시계열 데이터 (json)을 입력으로 받을 경우\n",
        "        model = make_model_efficientnet_1input()\n",
        "        net_name = '/content/drive/MyDrive/제4회 AI교육 해커톤/network.hdf5'\n",
        "        model.load_weights(net_name)\n",
        "\n",
        "        data = read_data_1_input(uploaded_file)\n",
        "        processed_data = preprocess_image(data, net_type='efficientnet')\n",
        "        prediction = model.predict(np.expand_dims(processed_data, axis=0), batch_size=1)\n",
        "        m_prediction = np.argmax(prediction)\n",
        "\n",
        "        # 결과 출력\n",
        "        st.write(f'Prediction is class {m_prediction}, => {CLASS[m_prediction]}')\n",
        "\n",
        "\n",
        "    elif file_extension in [\"jpg\", \"jpeg\", \"png\"]: # 이미지를 입력으로 받을 경우\n",
        "        try:\n",
        "            image = Image.open(uploaded_file)\n",
        "            st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "            result = process_image(image, model2)\n",
        "            m_prediction = CLASS.index(result)\n",
        "            st.success(\"이미지 파일이 성공적으로 처리되었습니다.\")\n",
        "            st.write(f'Prediction is class {m_prediction}, => {CLASS[m_prediction]}')\n",
        "        except Exception as e:\n",
        "            st.error(f\"이미지 파일을 처리하는 중 오류가 발생했습니다: {e}\")\n",
        "\n",
        "    else:\n",
        "        st.error(\"지원되지 않는 파일 형식입니다. JSON, JPG, JPEG 또는 PNG 파일을 업로드하세요.\")\n",
        "\n",
        "    # 예측한 결과에 따라 llm 설명 텍스트 출력\n",
        "    if m_prediction is not None:\n",
        "        st.title('상세 설명')\n",
        "        prediction = CLASS[m_prediction]\n",
        "        if prediction in llm_results:\n",
        "            st.write(llm_results[prediction])\n",
        "        else:\n",
        "            st.write(\"정상입니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z98Ko6Uz4Dlm",
        "outputId": "6a3ab1ea-b53e-4b2a-d64f-9b60a8aedc28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.169.81.153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 코드 실행해서 \"your url is\" 링크 들어가면 key 입력 필요 => 바로 위 코드에 있는 숫자 복붙 (이게 key)\n",
        "# 런타임 초기화되면 key 바뀜\n",
        "!streamlit run /content/app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4rlJsK84Puc",
        "outputId": "3e2efbbd-97d9-43f4-e6dc-58a4e39e6dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.169.81.153:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://few-goats-happen.loca.lt\n",
            "2024-10-13 18:38:57.432 Uncaught exception GET /_stcore/stream (127.0.0.1)\n",
            "HTTPServerRequest(protocol='http', host='few-goats-happen.loca.lt', method='GET', uri='/_stcore/stream', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/websocket.py\", line 937, in _accept_connection\n",
            "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/server/browser_websocket_handler.py\", line 126, in open\n",
            "    self._session_id = self._runtime.connect_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/runtime.py\", line 384, in connect_session\n",
            "    session_id = self._session_mgr.connect_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/websocket_session_manager.py\", line 99, in connect_session\n",
            "    session = AppSession(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 133, in __init__\n",
            "    self._pages_manager = PagesManager(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 236, in __init__\n",
            "    self.pages_strategy = PagesManager.DefaultStrategy(self, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 74, in __init__\n",
            "    PagesStrategyV1.watch_pages_dir(pages_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/pages_manager.py\", line 62, in watch_pages_dir\n",
            "    watch_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/path_watcher.py\", line 155, in watch_dir\n",
            "    return _watch_path(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/path_watcher.py\", line 126, in _watch_path\n",
            "    watcher_class(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 107, in __init__\n",
            "    path_watcher.watch_path(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 185, in watch_path\n",
            "    folder_handler.watch = self._observer.schedule(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/api.py\", line 312, in schedule\n",
            "    emitter.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/utils/__init__.py\", line 75, in start\n",
            "    self.on_thread_start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify.py\", line 119, in on_thread_start\n",
            "    self._inotify = InotifyBuffer(path, recursive=self.watch.is_recursive, event_mask=event_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_buffer.py\", line 30, in __init__\n",
            "    self._inotify = Inotify(path, recursive=recursive, event_mask=event_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 167, in __init__\n",
            "    self._add_dir_watch(path, event_mask, recursive=recursive)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 394, in _add_dir_watch\n",
            "    self._add_watch(full_path, mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 407, in _add_watch\n",
            "    Inotify._raise_error()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/watchdog/observers/inotify_c.py\", line 424, in _raise_error\n",
            "    raise OSError(err, os.strerror(err))\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Exception ignored in: <function AppSession.__del__ at 0x7b9394ee41f0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 174, in __del__\n",
            "    self.shutdown()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/app_session.py\", line 240, in shutdown\n",
            "    if self._state != AppSessionState.SHUTDOWN_REQUESTED:\n",
            "AttributeError: 'AppSession' object has no attribute '_state'\n",
            "2024-10-13 18:39:03.091613: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-13 18:39:03.113661: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-13 18:39:03.120567: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-13 18:39:03.136318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-13 18:39:04.270635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/content/app.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/content/app.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/content/app.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/content/app.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/content/app.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/content/app.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_0aUrgnZBsKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}